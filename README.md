# üöÄ Pipeline de An√°lise de Vendas - PySpark + MinIO

Sistema de ETL (Extract, Transform, Load) para an√°lise de dados de vendas usando PySpark e MinIO como data lake, executado em ambiente Jupyter Notebook.

---

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Configura√ß√£o](#configura√ß√£o)
- [Pipelines Implementados](#pipelines-implementados)
- [Como Executar](#como-executar)
- [Estrutura de Dados](#estrutura-de-dados)
- [Troubleshooting](#troubleshooting)

---

## üéØ Vis√£o Geral

Este projeto implementa **5 pipelines de transforma√ß√£o de dados** para an√°lise de vendas, processando dados de transa√ß√µes comerciais e gerando insights sobre:

- ‚úÖ Estat√≠sticas de vendas e lucro por categoria
- ‚úÖ An√°lise dos top 20 clientes por regi√£o
- ‚úÖ Foco espec√≠fico na regi√£o Central
- ‚úÖ C√°lculo de rentabilidade por cliente
- ‚úÖ Convers√£o de dados para formato anal√≠tico (long format)

### üé® Caracter√≠sticas

- **Processamento Distribu√≠do**: Utiliza Apache Spark para processar grandes volumes de dados
- **Storage S3-Compatible**: MinIO como data lake para armazenamento de dados
- **Formato Otimizado**: Dados salvos em Parquet para melhor performance
- **Logs Autom√°ticos**: Registro de execu√ß√£o de cada pipeline
- **Modular**: Cada pipeline √© uma fun√ß√£o independente

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dataset.csv   ‚îÇ
‚îÇ   (MinIO S3)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PySpark ETL    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇPipeline 1 ‚îÇ  ‚îÇ‚îÄ‚îÄ‚ñ∫ s3a://datalake/pipeline1/statistics
‚îÇ  ‚îÇPipeline 2 ‚îÇ  ‚îÇ‚îÄ‚îÄ‚ñ∫ s3a://datalake/pipeline2/top_clients_by_region
‚îÇ  ‚îÇPipeline 3 ‚îÇ  ‚îÇ‚îÄ‚îÄ‚ñ∫ s3a://datalake/pipeline3/central_sales
‚îÇ  ‚îÇPipeline 4 ‚îÇ  ‚îÇ‚îÄ‚îÄ‚ñ∫ s3a://datalake/pipeline4/profitability
‚îÇ  ‚îÇPipeline 5 ‚îÇ  ‚îÇ‚îÄ‚îÄ‚ñ∫ s3a://datalake/pipeline5/long_format
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MinIO S3      ‚îÇ
‚îÇ   (Data Lake)   ‚îÇ
‚îÇ   + Logs        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Pr√©-requisitos

### Software Necess√°rio

- **Python** 3.8+
- **Apache Spark** 3.3+
- **Jupyter Notebook** ou **JupyterLab**
- **MinIO** (servidor rodando)

### Bibliotecas Python

```bash
pip install pyspark
pip install jupyter
```

### Infraestrutura

- **MinIO Server** rodando em `http://minio:9000`
- **Bucket** `datalake` criado no MinIO
- **Dataset** `dataset.csv` carregado no bucket

---

## ‚öôÔ∏è Configura√ß√£o

### 1. Configura√ß√µes do MinIO

No notebook, ajuste as credenciais conforme seu ambiente:

```python
MINIO_ENDPOINT = "http://minio:9000"
ACCESS_KEY = "admin"      # Seu access key
SECRET_KEY = "password"   # Seu secret key
BUCKET_NAME = "datalake"
```

### 2. Estrutura do Dataset

O CSV deve conter as seguintes colunas:

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `Row ID` | Integer | Identificador √∫nico da linha |
| `Order ID` | String | ID do pedido |
| `Order Date` | Date | Data do pedido |
| `Ship Date` | Date | Data de envio |
| `Ship Mode` | String | Modo de envio |
| `Customer ID` | String | ID do cliente |
| `Customer Name` | String | Nome do cliente |
| `Segment` | String | Segmento do cliente |
| `Country` | String | Pa√≠s |
| `City` | String | Cidade |
| `State` | String | Estado |
| `Postal Code` | Integer | CEP |
| `Region` | String | Regi√£o (Central, East, South, West) |
| `Product ID` | String | ID do produto |
| `Category` | String | Categoria do produto |
| `Sub-Category` | String | Subcategoria |
| `Product Name` | String | Nome do produto |
| `Sales` | Double | Valor de vendas |
| `Quantity` | Integer | Quantidade vendida |
| `Discount` | Double | Desconto aplicado |
| `Profit` | Double | Lucro obtido |

---

## üìä Pipelines Implementados

### Pipeline 1: Estat√≠sticas de Vendas üìà

**Objetivo**: Calcular m√©dia e desvio padr√£o de vendas e lucro por subcategoria.

**Transforma√ß√µes**:
- Agrupamento por `Sub-Category`
- C√°lculo de m√©dia de `Profit` e `Sales`
- C√°lculo de desvio padr√£o de `Profit` e `Sales`
- Join dos resultados
- C√°lculo de diferen√ßa: `MeanProfit - StdProfit`

**Output**: `s3a://datalake/pipeline1/statistics`

**Colunas resultantes**:
```
Sub-Category | MeanProfit | MeanSales | StdProfit | StdSales | DiffMean
```

**Uso**:
```python
logs.append(pipeline_1_stats(df_raw))
```

---

### Pipeline 2: Top 20 Clientes por Regi√£o üèÜ

**Objetivo**: Identificar os 20 principais clientes e analisar suas vendas por regi√£o.

**Transforma√ß√µes**:
1. Identificar top 20 clientes globalmente (maior volume de vendas)
2. Filtrar dados apenas desses clientes
3. Criar pivot table: `Sub-Category` x `Region`
4. Calcular m√©dia de vendas
5. Renomear colunas para `Sales_{Region}`

**Output**: `s3a://datalake/pipeline2/top_clients_by_region`

**Estrutura**:
```
Sub-Category | Sales_Central | Sales_East | Sales_South | Sales_West
```

**Uso**:
```python
logs.append(pipeline_2_top_clients(df_raw))
```

---

### Pipeline 3: An√°lise Regi√£o Central üéØ

**Objetivo**: An√°lise focada nos top 20 clientes da regi√£o Central.

**Transforma√ß√µes**:
1. Filtrar apenas regi√£o `Central`
2. Identificar top 20 clientes dessa regi√£o
3. Pivot por `Customer ID` x `Sub-Category`
4. Somar vendas

**Output**: `s3a://datalake/pipeline3/central_sales`

**Estrutura**:
```
Customer ID | Accessories | Appliances | Art | Binders | ... | Tables
```

**Uso**:
```python
logs.append(pipeline_3_central(df_raw, spark))
```

---

### Pipeline 4: An√°lise de Rentabilidade üí∞

**Objetivo**: Calcular margem de lucro (profit ratio) por cliente e regi√£o.

**Transforma√ß√µes**:
1. Agrupar por `Region` e `Customer ID`
2. Somar `Sales` e `Profit`
3. Calcular `profitRatio = Profit / Sales`
4. Ordenar por rentabilidade decrescente

**Output**: `s3a://datalake/pipeline4/profitability`

**Colunas resultantes**:
```
Region | Customer ID | Sales | Profit | profitRatio
```

**Uso**:
```python
logs.append(pipeline_4_profitability(df_raw))
```

**Interpreta√ß√£o**:
- `profitRatio > 0`: Cliente lucrativo
- `profitRatio < 0`: Cliente com preju√≠zo
- `profitRatio = 0.25`: 25% de margem de lucro

---

### Pipeline 5: Formato Long (Unpivot) üìù

**Objetivo**: Converter dados de formato wide para long (an√°lise mais flex√≠vel).

**Transforma√ß√µes**:
1. Agregar dados por `Region` e `Customer ID`
2. Usar fun√ß√£o `stack()` para unpivot
3. Criar colunas `Metric` e `Value`

**Output**: `s3a://datalake/pipeline5/long_format`

**Estrutura**:
```
Region | Customer ID | Metric       | Value
-------+--------------+--------------+--------
West   | AB-10001     | Sales        | 1250.50
West   | AB-10001     | Profit       | 312.75
West   | AB-10001     | ProfitRatio  | 0.25
```

**Uso**:
```python
logs.append(pipeline_5_long_format(df_raw))
```

---

## üöÄ Como Executar

### Op√ß√£o 1: Jupyter Notebook (Passo a Passo)

1. **Abrir o notebook**:
```bash
jupyter notebook pipeline.ipynb
```

2. **Executar c√©lulas sequencialmente**:
   - Cell 1: Imports
   - Cell 2: Configura√ß√£o do Spark
   - Cell 3: Fun√ß√£o de carga de dados
   - Cells 4-10: Fun√ß√µes dos pipelines
   - Cell 11: Fun√ß√£o de log
   - Cell 12: Execu√ß√£o principal

3. **Resultado esperado**:
```
Iniciando Spark .
Lendo dados de origem: s3a://datalake/dataset.csv
Registros carregados: 9994
Pipeline 1: Estat√≠sticas (M√©dia/Desvio Padr√£o)...
Pipeline 2: Top 20 Clientes por Regi√£o
Pipeline 3: An√°lise Regi√£o Central
Pipeline 4: Rentabilidade
Pipeline 5: Formato Longo (Stack)...
Salvando Logs
‚úÖ SUCESSO! Todos os pipelines foram executados e salvos no MinIO.
```

### Op√ß√£o 2: Script Python

Converter o notebook para script:

```bash
jupyter nbconvert --to script pipeline.ipynb
python pipeline.py
```

### Op√ß√£o 3: Spark Submit

```bash
spark-submit \
  --master local[*] \
  --packages org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
  --conf spark.hadoop.fs.s3a.access.key=admin \
  --conf spark.hadoop.fs.s3a.secret.key=password \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  pipeline.py
```

---

## üìÅ Estrutura de Dados no MinIO

Ap√≥s execu√ß√£o, a seguinte estrutura √© criada no bucket `datalake`:

```
datalake/
‚îú‚îÄ‚îÄ dataset.csv                          # Dados originais
‚îú‚îÄ‚îÄ pipeline1/
‚îÇ   ‚îî‚îÄ‚îÄ statistics/
‚îÇ       ‚îú‚îÄ‚îÄ _SUCCESS
‚îÇ       ‚îî‚îÄ‚îÄ part-*.parquet
‚îú‚îÄ‚îÄ pipeline2/
‚îÇ   ‚îî‚îÄ‚îÄ top_clients_by_region/
‚îÇ       ‚îú‚îÄ‚îÄ _SUCCESS
‚îÇ       ‚îî‚îÄ‚îÄ part-*.parquet
‚îú‚îÄ‚îÄ pipeline3/
‚îÇ   ‚îî‚îÄ‚îÄ central_sales/
‚îÇ       ‚îú‚îÄ‚îÄ _SUCCESS
‚îÇ       ‚îî‚îÄ‚îÄ part-*.parquet
‚îú‚îÄ‚îÄ pipeline4/
‚îÇ   ‚îî‚îÄ‚îÄ profitability/
‚îÇ       ‚îú‚îÄ‚îÄ _SUCCESS
‚îÇ       ‚îî‚îÄ‚îÄ part-*.parquet
‚îú‚îÄ‚îÄ pipeline5/
‚îÇ   ‚îî‚îÄ‚îÄ long_format/
‚îÇ       ‚îú‚îÄ‚îÄ _SUCCESS
‚îÇ       ‚îî‚îÄ‚îÄ part-*.parquet
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ _SUCCESS
    ‚îî‚îÄ‚îÄ part-*.parquet
```

### Metadados de Logs

O arquivo de logs cont√©m:

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `pipeline` | String | Nome do pipeline (Pipeline1, Pipeline2, etc.) |
| `path` | String | Caminho completo no MinIO |
| `count` | Long | N√∫mero de registros gerados |
| `timestamp` | Timestamp | Data/hora de execu√ß√£o |

---

## üîç Verifica√ß√£o de Resultados

### Via MinIO Console

1. Acessar: `http://localhost:9001` (ou IP do servidor)
2. Login com credenciais configuradas
3. Navegar at√© bucket `datalake`
4. Explorar pastas dos pipelines

### Via Jupyter Notebook

```python
# Ler resultado do Pipeline 1
df_result = spark.read.parquet("s3a://datalake/pipeline1/statistics")
df_result.show(10)

# Contar registros
print(f"Total de registros: {df_result.count()}")

# Ver schema
df_result.printSchema()

# Converter para Pandas (para visualiza√ß√£o)
pdf = df_result.toPandas()
print(pdf.head())
```

### Via CLI do MinIO

```bash
# Listar arquivos
mc ls myminio/datalake/pipeline1/statistics/

# Download de arquivo
mc cp myminio/datalake/pipeline1/statistics/part-00000.parquet ./
```

---

## üìä Exemplos de An√°lise

### 1. Top 5 Subcategorias Mais Lucrativas

```python
df_stats = spark.read.parquet("s3a://datalake/pipeline1/statistics")
df_stats.orderBy(desc("MeanProfit")).show(5)
```

### 2. Clientes com Melhor Margem de Lucro

```python
df_prof = spark.read.parquet("s3a://datalake/pipeline4/profitability")
df_prof.filter(col("profitRatio") > 0.3).count()
```

### 3. Vendas por Regi√£o (Top Clientes)

```python
df_region = spark.read.parquet("s3a://datalake/pipeline2/top_clients_by_region")
df_region.select("Sub-Category", "Sales_Central", "Sales_East").show()
```

---

## üõ†Ô∏è Troubleshooting

### Problema 1: Erro de Conex√£o com MinIO

**Erro**:
```
NoSuchBucket: The specified bucket does not exist
```

**Solu√ß√£o**:
```python
# Criar bucket via c√≥digo
from minio import Minio

client = Minio(
    "minio:9000",
    access_key="admin",
    secret_key="password",
    secure=False
)

if not client.bucket_exists("datalake"):
    client.make_bucket("datalake")
```

### Problema 2: Arquivo CSV N√£o Encontrado

**Erro**:
```
Path does not exist: s3a://datalake/dataset.csv
```

**Solu√ß√£o**:
1. Verificar se arquivo foi carregado no MinIO
2. Usar MinIO Console para fazer upload
3. Ou usar CLI:
```bash
mc cp dataset.csv myminio/datalake/
```

### Problema 3: Mem√≥ria Insuficiente

**Erro**:
```
OutOfMemoryError: Java heap space
```

**Solu√ß√£o**:
```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()
```

### Problema 4: Depend√™ncias Faltando

**Erro**:
```
java.lang.ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem
```

**Solu√ß√£o**:
```bash
# Download manual do JAR
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar

# Adicionar ao Spark
spark = SparkSession.builder \
    .config("spark.jars", "/path/to/hadoop-aws-3.3.4.jar") \
    .getOrCreate()
```

### Problema 5: Logs Muito Verbosos

**Solu√ß√£o**:
```python
spark.sparkContext.setLogLevel("ERROR")
# ou
spark.sparkContext.setLogLevel("WARN")
```

---

## üìà M√©tricas de Performance

### Dataset de Exemplo (9,994 registros)

| Pipeline | Tempo M√©dio | Registros Sa√≠da | Tamanho Parquet |
|----------|-------------|-----------------|-----------------|
| Pipeline 1 | ~2s | 17 | ~5 KB |
| Pipeline 2 | ~3s | 17 | ~8 KB |
| Pipeline 3 | ~2s | 20 | ~15 KB |
| Pipeline 4 | ~2s | 2,501 | ~120 KB |
| Pipeline 5 | ~2s | 7,503 | ~180 KB |

**Total**: ~11 segundos para processamento completo

---

## üîê Seguran√ßa

### Boas Pr√°ticas

1. **N√£o commitar credenciais**: Use vari√°veis de ambiente
```python
import os
ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY")
SECRET_KEY = os.getenv("MINIO_SECRET_KEY")
```

2. **Criptografia**: Habilitar SSL no MinIO
```python
.config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true")
```

3. **Controle de acesso**: Usar IAM policies no MinIO

---


## üìö Refer√™ncias

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [MinIO Python Client](https://min.io/docs/minio/linux/developers/python/API.html)
- [Hadoop AWS Integration](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html)

---




