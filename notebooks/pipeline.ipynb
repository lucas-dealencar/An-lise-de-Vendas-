{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80fd595-5a0a-48ea-bdb0-5a28a0a2a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, sum as _sum, desc, count, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c33c11-2978-411f-8524-82938ded4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "ACCESS_KEY = \"admin\"      \n",
    "SECRET_KEY = \"password\"  \n",
    "BUCKET_NAME = \"datalake\"\n",
    "\n",
    "# Caminhos\n",
    "INPUT_PATH = f\"s3a://{BUCKET_NAME}/dataset.csv\"\n",
    "BASE_OUTPUT_PATH = f\"s3a://{BUCKET_NAME}\"\n",
    "\n",
    "\n",
    "def get_spark_session():\n",
    "    print(\"Iniciando Spark .\")\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Sales_Analysis_ETL_Final\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e18af62f-38f0-43ca-a96e-6ec2a592a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark):\n",
    "    print(f\"\\nLendo dados de origem: {INPUT_PATH}\")\n",
    "    # Lê CSV inferindo schema \n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(INPUT_PATH)\n",
    "    \n",
    "    print(f\"Registros carregados: {df.count()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b24ba1-24f8-4101-8e04-7b50c41d088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac8bb93-8954-4ff2-92d0-04303283bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_1_stats(df):\n",
    "    \"\"\"Estatísticas de Vendas e Lucro por Sub-Categoria\"\"\"\n",
    "    print(\"Pipeline 1: Estatísticas (Média/Desvio Padrão)...\")\n",
    "    \n",
    "    df_media = df.groupBy(\"Sub-Category\").agg(mean(\"Profit\").alias(\"MeanProfit\"), mean(\"Sales\").alias(\"MeanSales\"))\n",
    "    df_std = df.groupBy(\"Sub-Category\").agg(stddev(\"Profit\").alias(\"StdProfit\"), stddev(\"Sales\").alias(\"StdSales\"))\n",
    "    \n",
    "    df_final = df_media.join(df_std, on=\"Sub-Category\", how=\"left\") \\\n",
    "        .withColumn(\"DiffMean\", col(\"MeanProfit\") - col(\"StdProfit\")) \\\n",
    "        .orderBy(desc(\"DiffMean\"))\n",
    "        \n",
    "    path = f\"{BASE_OUTPUT_PATH}/pipeline1/statistics\"\n",
    "    df_final.write.mode(\"overwrite\").parquet(path)\n",
    "    return (\"Pipeline1\", path, df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcce09-ba1b-4013-a985-2dd9cc95367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b11e2f-4f06-4191-b0ff-7c33cea49778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_2_top_clients(df):\n",
    "    \"\"\"Top 20 Clientes pivotado por Região\"\"\"\n",
    "    print(\"Pipeline 2: Top 20 Clientes por Região\")\n",
    "    \n",
    "    #Top 20 Global\n",
    "    top_clients = df.groupBy(\"Customer ID\").agg(_sum(\"Sales\").alias(\"TotalSales\")).orderBy(desc(\"TotalSales\")).limit(20)\n",
    "    top_ids = [row[\"Customer ID\"] for row in top_clients.collect()]\n",
    "    \n",
    "    # 2. Filtrar e Pivotar\n",
    "    df_filtered = df.filter(col(\"Customer ID\").isin(top_ids))\n",
    "    df_pivot = df_filtered.groupBy(\"Sub-Category\").pivot(\"Region\").agg(mean(\"Sales\"))\n",
    "    \n",
    "    \n",
    "    for c in df_pivot.columns:\n",
    "        if c != \"Sub-Category\":\n",
    "            df_pivot = df_pivot.withColumnRenamed(c, f\"Sales_{c}\")\n",
    "\n",
    "    path = f\"{BASE_OUTPUT_PATH}/pipeline2/top_clients_by_region\"\n",
    "    df_pivot.write.mode(\"overwrite\").parquet(path)\n",
    "    return (\"Pipeline2\", path, df_pivot.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd13682-d62c-425c-b421-e64de5d780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8981be06-b1c1-4467-9417-567e2788ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_3_central(df, spark):\n",
    "    \"\"\"Foco na Região Central\"\"\"\n",
    "    print(\"Pipeline 3: Análise Região Central\")\n",
    "    \n",
    "    # Filtrar e Top 20 Central\n",
    "    df_central = df.filter(col(\"Region\") == \"Central\")\n",
    "    top_central = df_central.groupBy(\"Customer ID\").agg(_sum(\"Sales\").alias(\"S\")).orderBy(desc(\"S\")).limit(20)\n",
    "    ids_central = [row[\"Customer ID\"] for row in top_central.collect()]\n",
    "    \n",
    "    # Pivot\n",
    "    df_data = df_central.filter(col(\"Customer ID\").isin(ids_central))\n",
    "    df_pivot = df_data.groupBy(\"Customer ID\").pivot(\"Sub-Category\").agg(_sum(\"Sales\"))\n",
    "    \n",
    "    path = f\"{BASE_OUTPUT_PATH}/pipeline3/central_sales\"\n",
    "    df_pivot.write.mode(\"overwrite\").parquet(path)\n",
    "    return (\"Pipeline3\", path, df_pivot.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3598e57-75ee-4476-bee4-7b04e71a2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2a66e2-5cc0-4712-9997-373974f0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_4_profitability(df):\n",
    "    \"\"\"Rentabilidade (Profit Ratio)\"\"\"\n",
    "    print(\"Pipeline 4: Rentabilidade\")\n",
    "    \n",
    "    df_prof = df.groupBy(\"Region\", \"Customer ID\") \\\n",
    "        .agg(_sum(\"Sales\").alias(\"Sales\"), _sum(\"Profit\").alias(\"Profit\")) \\\n",
    "        .withColumn(\"profitRatio\", col(\"Profit\") / col(\"Sales\")) \\\n",
    "        .orderBy(desc(\"profitRatio\"))\n",
    "        \n",
    "    path = f\"{BASE_OUTPUT_PATH}/pipeline4/profitability\"\n",
    "    df_prof.write.mode(\"overwrite\").parquet(path)\n",
    "    return (\"Pipeline4\", path, df_prof.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87d678-bf18-4ee4-98b3-678416d9c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fa5324-04a7-42eb-aa5f-67934bd2c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_5_long_format(df):\n",
    "    \"\"\"Unpivot para formato Longo\"\"\"\n",
    "    print(\"Pipeline 5: Formato Longo (Stack)...\")\n",
    "    \n",
    "    df_base = df.groupBy(\"Region\", \"Customer ID\") \\\n",
    "                .agg(_sum(\"Sales\").alias(\"Sales\"), _sum(\"Profit\").alias(\"Profit\")) \\\n",
    "                .withColumn(\"profitRatio\", col(\"Profit\") / col(\"Sales\"))\n",
    "    \n",
    "    # Transforma colunas em linhas\n",
    "    df_long = df_base.select(\n",
    "        \"Region\", \"Customer ID\",\n",
    "        F.expr(\"stack(3, 'Sales', Sales, 'Profit', Profit, 'ProfitRatio', profitRatio) as (Metric, Value)\")\n",
    "    )\n",
    "    \n",
    "    path = f\"{BASE_OUTPUT_PATH}/pipeline5/long_format\"\n",
    "    df_long.write.mode(\"overwrite\").parquet(path)\n",
    "    return (\"Pipeline5\", path, df_long.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7cd35-a2dd-42b1-8816-af1ab85fa06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16878550-881a-4231-b6a9-4ed2f1bd8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logs(spark, logs):\n",
    "    \"\"\"Salva metadados da execução\"\"\"\n",
    "    print(\"\\nSalvando Logs\")\n",
    "    schema = StructType([\n",
    "        StructField(\"pipeline\", StringType()), StructField(\"path\", StringType()), \n",
    "        StructField(\"count\", LongType()), StructField(\"timestamp\", TimestampType())\n",
    "    ])\n",
    "    data = [(l[0], l[1], l[2], datetime.now()) for l in logs]\n",
    "    spark.createDataFrame(data, schema).write.mode(\"overwrite\").parquet(f\"{BASE_OUTPUT_PATH}/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10df3-6c6e-4a09-bf1a-70e4482b2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba39daca-e187-4315-b3c2-627dce29ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Spark .\n",
      "\n",
      "Lendo dados de origem: s3a://datalake/dataset.csv\n",
      "Registros carregados: 9994\n",
      "Pipeline 1: Estatísticas (Média/Desvio Padrão)...\n",
      "Pipeline 2: Top 20 Clientes por Região\n",
      "Pipeline 3: Análise Região Central\n",
      "Pipeline 4: Rentabilidade\n",
      "Pipeline 5: Formato Longo (Stack)...\n",
      "\n",
      "Salvando Logs\n",
      "\n",
      "✅ SUCESSO! Todos os pipelines foram executados e salvos no MinIO.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    try:\n",
    "        df_raw = load_data(spark)\n",
    "        \n",
    "        logs = []\n",
    "        logs.append(pipeline_1_stats(df_raw))\n",
    "        logs.append(pipeline_2_top_clients(df_raw))\n",
    "        logs.append(pipeline_3_central(df_raw, spark))\n",
    "        logs.append(pipeline_4_profitability(df_raw))\n",
    "        logs.append(pipeline_5_long_format(df_raw))\n",
    "        \n",
    "        # 3. Finalizar\n",
    "        save_logs(spark, logs)\n",
    "        print(\"\\n✅ SUCESSO! Todos os pipelines foram executados e salvos no MinIO.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ff415-a63e-4aa2-bb12-c579d7557e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
